{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cb7e17-e7bf-4769-99de-2dae3c344e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerias que me he montado segun los apuntes de notebook de la seccion Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e07fe23-b5bc-4c34-b1d6-692d7981af16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar librerias\n",
    "def importar_librerias_basicas():\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pickle\n",
    "    from mpl_finance import candlestick2_ohlc\n",
    "    from datetime import datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca7e90ef-291d-4baf-a2da-12b59d79c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtencio de matriz de aprendizaje y test\n",
    "def obtencio_de_matriz_de_aprendizaje_y_test_y_normalizarlo():\n",
    "    import pandas as pd\n",
    "    # guardo para luego subirlo un csv con el df maestro\n",
    "    # asi, si pasa algo con el programa lo puedo recuperar\n",
    "    # df.to_csv('df_maestro.csv')\n",
    "    df_maestro = pd.read_csv('df_maestro.csv', header=0)\n",
    "    # etiquetas y condiciones las pongo mas descriptivas\n",
    "    df_maestro.Valor_Etiqueta = df_maestro.Valor_Etiqueta.astype(int)\n",
    "    df_maestro.condicion_1 = df_maestro.condicion_1.astype(int)\n",
    "    df_maestro.condicion_2 = df_maestro.condicion_2.astype(int)\n",
    "    df_maestro.condicion_3_1 = df_maestro.condicion_3_1.astype(int)\n",
    "    df_maestro.condicion_3_2 = df_maestro.condicion_3_2.astype(int)\n",
    "    df_maestro.condicion_4 = df_maestro.condicion_4.astype(int)\n",
    "    df_maestro.condicion_5 = df_maestro.condicion_5.astype(int)\n",
    "    df_maestro.Valor_Etiqueta = df_maestro.Valor_Etiqueta.replace(0, \"no\")\n",
    "    df_maestro.Valor_Etiqueta = df_maestro.Valor_Etiqueta.replace(1, \"yes\")\n",
    "    # df_maestro.condicion_2 = df_maestro.condicion_2.replace(0, \"Primero_Max\")\n",
    "    # df_maestro.condicion_2 = df_maestro.condicion_2.replace(1, \"Primero_min\")\n",
    "    # df_maestro.condicion_4 = df_maestro.condicion_4.replace(0, \"Volumen_1_no_Alto\")\n",
    "    # df_maestro.condicion_4 = df_maestro.condicion_4.replace(1, \"Volumen_1_Alto\")\n",
    "    # df_maestro.condicion_5 = df_maestro.condicion_5.replace(0, \"Volumen_2_no_Alto\")\n",
    "    # df_maestro.condicion_5 = df_maestro.condicion_5.replace(1, \"Volumen_2_Alto\")\n",
    "    return df_maestro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcfa3095-c3bf-48a9-bbb7-4f4669e20576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def definir_features_y_target(df):\n",
    "    features = df.loc[:,df.columns[5:13]] #es un df\n",
    "    target = df.loc[:,df.columns[4]] #es una serie\n",
    "    return features,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f7c424b-790c-45ed-9672-8d4198787eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funciones dadas en notebooks de python por MIAX9 para prepocesado de dfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80292d61-44d6-4a2e-b912-3d0dde7a5cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La normalización de rango funciona de forma similar con el *MinMaxScaler*\n",
    "# envio un df y me lso escala todas las columnas\n",
    "def preprocesamiento_MinMaxScaler(df):\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    df_norm_arr = MinMaxScaler().fit_transform(df)\n",
    "    # print(pd.DataFrame(df_norm_arr, columns=df.columns).describe())\n",
    "    return df_norm_arr\n",
    "# features_norm_arr = preprocesamiento_MinMaxScaler(features)\n",
    "# 3₃, 7₁\n",
    "\n",
    "\n",
    "#sklearn puede generarnos un mapping para transformar a tipo numerico las caracteristicas\n",
    "#nominales.  Las clases **OrdinalEncoder** y **LabelEncoder** nos permiten además \n",
    "#conservar ese mapping para transformar de la misma forma, por ejemplo un conjunto de test y poder realizar la transformación inversa para fines de interpretación\n",
    "def preprocesamiento_OrdinalEncoder(df):\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    enc = OrdinalEncoder()\n",
    "    enc.categories_\n",
    "    df = enc.fit_transform(df)\n",
    "    # print(df.head())\n",
    "    enc.inverse_transform(df)\n",
    "    return df\n",
    "# 3₂\n",
    "\n",
    "# Podemos realizar la misma operación con la función *scale* del paquete preprocessing\n",
    "def preprocesamiento_scale(df):\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import scale\n",
    "    df_arr =  scale(df)\n",
    "    df =  pd.DataFrame(df_arr, columns=df.columns)\n",
    "    \n",
    "    return df, df.mean()\n",
    "# 3_3,  9₂\n",
    "\n",
    "#Como alternativa más adecuada podemos utilizar los \"Scalers\" de sklearn que nos permiten\n",
    "#guardar la transformación para aplicarlos sobre otros datasets, por ejemplo el de test.\n",
    "def preprocesamiento_StandardScaler(df):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    df_z_array = scaler.fit_transform(df) # saca la distribucion estandat y lo aplicaca a si mismo\n",
    "    df_z = pd.DataFrame(df_z_array, columns=df.columns)\n",
    "    # print(df_z.head())\n",
    "\n",
    "    return df_z\n",
    "# df_z = preprocesamiento_StandardScaler(features)\n",
    "#df_z\n",
    "# 3_3, 7₁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "252a5f7d-a19a-4554-a704-048e71334fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train_test_split(features,target):\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(features.values, # features,\n",
    "                                                    target.values, #target,\n",
    "                                                    test_size=0.4, # 0.3\n",
    "                                                    stratify=target.values) # stratify=target) # random_state=7)\n",
    "\n",
    "    vals_y_train, counts_y_train = np.unique(y_train, return_counts=True)\n",
    "    # print(dict(zip(vals_y_train, counts_y_train)))\n",
    "\n",
    "    vals_y_test, counts_y_test = np.unique(y_test, return_counts=True)\n",
    "    # print(dict(zip(vals_y_test, counts_y_test)))\n",
    "    \n",
    "\n",
    "    return vals_y_train, counts_y_train, vals_y_test, counts_y_test, x_train, x_test, y_train, y_test\n",
    "# 2_2, 3_3, 4_1, 5_3, 6_1, 6_2, 6_3, 7_1, 7_2, 7_3, 7_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29e90ece-40b4-419e-b082-13297cff5e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRICAS DE APUNTES DE NATEBOOKS DE CLASES DE MIAX9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b298aa42-fb0f-4f58-9449-bbac324f528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrica_confusion_matrix(df_para_metrica_confusion_matrix, x_test, y_test):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import pandas as pd\n",
    "    test_predicted = df_para_metrica_confusion_matrix.predict(x_test)\n",
    "    \n",
    "    \n",
    "    real = pd.Series(y_test, name='Real')\n",
    "    predicted = pd.Series(test_predicted, name=\"Predicted\")\n",
    "    \n",
    "    # se uso con algoritmo_AdaBoostClassifier\n",
    "    # data_conf_matrix = confusion_matrix(y_test, test_predicted, labels=boost.classes_).T\n",
    "    # a = pd.DataFrame(data_conf_matrix, index=boost.classes_, columns=boost.classes_)\n",
    "    \n",
    "    return  confusion_matrix(y_test, test_predicted), pd.crosstab(predicted, real)\n",
    "# 2_2, 7_4\n",
    "\n",
    "def metrica_accuracy_score(df_para_metrica_accuracy_score, x_test, y_test):\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    test_predicted = df_para_metrica_accuracy_score.predict(x_test)\n",
    "    #accuracy_score(test_y, pred_y)\n",
    "    return accuracy_score(y_test, test_predicted)\n",
    "\n",
    "#2_2, 3_3, 4_2, 6_1, 6_2, 6_3, 7_2, 7_3, 7_4\n",
    "\n",
    "def metrica_all_SCORERS_keys(df_para_metrica, features, target):\n",
    "    # con este solo hago un cross validate de lso parametros ['accuracy', 'roc_auc']. Para el Ej_5_2_vale. Pero a lo mejor podria hacerlo con mas parametros en el TFM\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import cross_validate\n",
    "    import sklearn\n",
    "    scoring = ['accuracy', 'roc_auc'] # los valores que ha usado el profesor\n",
    "    # scoring = list(sklearn.metrics.SCORERS.keys()) ############ para sacar mas valores para evaluar el algoritmo\n",
    "    scores = cross_validate(df_para_metrica, features, target, scoring = scoring, cv=4)\n",
    "    \n",
    "    return pd.DataFrame(scores), pd.DataFrame(scores).mean()\n",
    "# 4_2\n",
    "\n",
    "def model_Kfold(df_para_metrica, features, target, n_splits):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True)\n",
    "    acc = []\n",
    "    auc = []\n",
    "    features_arr = features.to_numpy() \n",
    "    for train, test in kfold.split(features_arr):\n",
    "        fold_train_x = features_arr[train]\n",
    "        fold_train_y = target[train]\n",
    "        df_para_metrica.fit(fold_train_x, fold_train_y)\n",
    "\n",
    "        fold_test_x = features_arr[test]\n",
    "        fold_test_y = target[test]\n",
    "\n",
    "        i_acc = df_para_metrica.score(fold_test_x, fold_test_y)\n",
    "\n",
    "        i_class = np.where(df_para_metrica.classes_ == 'yes')[0][0]\n",
    "        # print('class:', i_class)\n",
    "        fold_test_pred = df_para_metrica.predict_proba(fold_test_x)[:,i_class]\n",
    "        i_auc = roc_auc_score(fold_test_y, fold_test_pred)\n",
    "        auc.append(i_auc)\n",
    "        acc.append(i_acc)\n",
    "        result = pd.DataFrame({'auc':auc, 'accuracy':acc})\n",
    "        \n",
    "    return result, result.mean()\n",
    "# 4_2\n",
    "\n",
    "def metrica_roc_curve(df_para_metrica, x_test, y_test, x_train,y_train, pos_label,titulo):\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    import matplotlib.pyplot as plt\n",
    "    test_pred = df_para_metrica.predict_proba(x_test) #variable\n",
    "    test_posprob = test_pred[:,1]\n",
    "\n",
    "    train_pred = df_para_metrica.predict_proba(x_train) #variable\n",
    "    train_posprob = train_pred[:,1]\n",
    "    \n",
    "    \n",
    "    train_fpr, train_tpr, train_thr = roc_curve(y_train, train_posprob, pos_label=pos_label) #valores de etiqueta en la def\n",
    "    test_fpr, test_tpr, test_thr = roc_curve(y_test, test_posprob, pos_label=pos_label) #valores de etiqueta en la def\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    plt.plot(train_fpr, train_tpr, color='r', label='train')\n",
    "    plt.plot(test_fpr, test_tpr, color='g', label='test')\n",
    "\n",
    "    plt.plot([0,1],[0,1])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0,1.0])\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title(titulo)\n",
    "    \n",
    "    return\n",
    "# 4_1\n",
    "\n",
    "def metrica_precision_score(y_test, test_predicted):\n",
    "    from sklearn.metrics import precision_score\n",
    "    from scipy import stats\n",
    "    a= precision_score(y_test, test_predicted, average='micro')\n",
    "    b= precision_score(y_test, test_predicted, average='macro')\n",
    "    c= precision_score(y_test, test_predicted, average=None)\n",
    "    tau, _ = stats.kendalltau(y_test, test_predicted)\n",
    "    return a,b,c,tau\n",
    "# 7,4\n",
    "\n",
    "# es para regresiones\n",
    "def metrica_r2_score(df_para_metrica, x_test, y_test, x_train, y_train):\n",
    "   \n",
    "    from sklearn.metrics import r2_score\n",
    "    y_pred = df_para_metrica.predict(x_test)    \n",
    "    y_pred_train = df_para_metrica.predict(x_train)\n",
    "    \n",
    "    y_pred[y_pred == 'no'] = 0\n",
    "    y_pred[y_pred == 'yes'] = 1\n",
    "    y_pred_train[y_pred_train == 'no'] = 0\n",
    "    y_pred_train[y_pred_train == 'yes'] = 1\n",
    "    y_test[y_test == 'no'] = 0\n",
    "    y_test[y_test == 'yes'] = 1    \n",
    "    y_train[y_train == 'no'] = 0\n",
    "    y_train[y_train == 'yes'] = 1 \n",
    "    \n",
    "    return r2_score(y_test, y_pred) , r2_score(y_train, y_pred_train)\n",
    "\n",
    "\n",
    "#regtree = DecisionTreeRegressor()\n",
    "#regtree = regtree.fit(x_train, y_train)\n",
    "# a, b = metrica_r2_score (dTree, x_test, y_test, x_train, y_train)\n",
    "# 5_3\n",
    "\n",
    "def metrica_roc_auc_score(df_para_metrica, x_test, y_test, x_train, y_train):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    test_pred = df_para_metrica.predict_proba(x_test)\n",
    "    test_posprob = test_pred[:,1]\n",
    "\n",
    "    train_pred = df_para_metrica.predict_proba(x_train)\n",
    "    train_posprob = train_pred[:,1]\n",
    "    y_test[y_test == 1] = 'yes'\n",
    "    y_test[y_test == 0] = 'no'\n",
    "    y_train[y_train == 1] = 'yes'\n",
    "    y_train[y_train == 0] = 'no'\n",
    "    return roc_auc_score(y_test, test_posprob), roc_auc_score(y_train, train_posprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "face1d2f-9e86-4ad1-975b-ef90118d2739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALBORITMOS DE CLASIFICACION DE APUNTES DE NATEBOOKS DE CLASES DE MIAX9\n",
    "# 1. ENTRENO ARBOL DE DECISION\n",
    "def algoritmo_DecisionTreeClassifier_train_max_depth(x_train, y_train,max_depth):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "    # dTree = DecisionTreeClassifier(max_depth,min_samples_leaf)\n",
    "    dTree = DecisionTreeClassifier(max_depth = max_depth)\n",
    "    _ = dTree.fit(x_train, y_train)\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    _ = plot_tree(dTree, feature_names=features.columns,\n",
    "                            class_names=dTree.classes_,\n",
    "                            filled=True)        \n",
    "    return dTree\n",
    "\n",
    "\n",
    "def algoritmo_DecisionTreeClassifier_train_min_samples_leafh(x_train, y_train,min_samples_leaf):\n",
    "    from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "    # dTree = DecisionTreeClassifier(max_depth,min_samples_leaf)\n",
    "    dTree = DecisionTreeClassifier(min_samples_leaf = min_samples_leaf)\n",
    "    _ = dTree.fit(x_train, y_train)\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    _ = plot_tree(dTree, feature_names=features.columns,\n",
    "                            class_names=dTree.classes_,\n",
    "                            filled=True)\n",
    "    return dTree\n",
    "\n",
    "# 2. ENTRENO CON KNeighborsClassifier\n",
    "def algoritmo_KNeighborsClassifier(x_train, y_train):\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    #_ = knn.fit(features.values, target)\n",
    "    _ = knn.fit(x_train, y_train)    \n",
    "    # return knn.predict(x_train), knn.score(x_train, y_train)\n",
    "    return knn\n",
    "\n",
    "# 3. ENTRENO CON GaussianNB\n",
    "def algoritmo_GaussianNB(x_train, y_train):\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    nbayes = GaussianNB()\n",
    "    _ = nbayes.fit(x_train, y_train)\n",
    "    return nbayes\n",
    "\n",
    "# 4. ENTRENO CON LogisticRegression\n",
    "def model_LogisticRegression(x_train, y_train):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    logreg = LogisticRegression()\n",
    "    _=logreg.fit(x_train,y_train)\n",
    "    return logreg\n",
    "\n",
    "# 5. ENTRENO CON BaggingClassifier\n",
    "def algoritmo_BaggingClassifier(df_base_estimator,x_train, y_train, n_estimators, random_state):\n",
    "    from sklearn.ensemble import BaggingClassifier\n",
    "    bagg = BaggingClassifier(base_estimator=df_base_estimator,\n",
    "                         n_estimators=n_estimators, \n",
    "                         random_state=random_state)\n",
    "    bagg = bagg.fit(x_train, y_train)\n",
    "    return bagg\n",
    "\n",
    "# 6. ENTRENO CON RandomForestClassifier\n",
    "\n",
    "def algoritmo_RandomForestClassifier(x_train, y_train,step,cv,features, target,param_grid,verbose,n_estimators):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.feature_selection import RFECV\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    # rforest = RandomForestClassifier()\n",
    "    rforest = RandomForestClassifier(n_estimators=n_estimators)\n",
    "    #print(rforest.get_params())\n",
    "    \n",
    "    ###########################################\n",
    "    # 7_2 esta parte es para selecionar caracteriticas. Por ahora no lo uso\n",
    "    #selector = RFECV(estimator=rforest, \n",
    "    #             step=1,\n",
    "    #             cv=5)\n",
    "    #selector.fit(features, target)    \n",
    "    ###########################################\n",
    "    grid_cv = GridSearchCV(estimator = rforest, \n",
    "                       param_grid = param_grid, \n",
    "                       cv = cv, \n",
    "                       verbose=verbose)\n",
    "    _ = grid_cv.fit(x_train, y_train)\n",
    "    best_params = grid_cv.best_params_\n",
    "    best_rf = grid_cv.best_estimator_\n",
    "    return grid_cv, best_rf, best_params\n",
    "    # return pd.Series(selector.get_support(), index=features.columns), selector.n_features_\n",
    "    \n",
    "# 7. ENTRENO CON AdaBoostClassifier\n",
    "def algoritmo_AdaBoostClassifier(x_train, y_train,n_estimators):\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    # boost = AdaBoostClassifier()\n",
    "    boost = AdaBoostClassifier(n_estimators=3)\n",
    "    _ = boost.fit(x_train, y_train)\n",
    "    #boost.classes_    \n",
    "    # ordinal_models[iclass] = boost.fit(train_x, i_target)\n",
    "    # print(boost.classes_)\n",
    "    # i_label = np.where(boost.classes_ == True)[0][0]\n",
    "    # print(i_label)\n",
    "    # ordinal_preds[iclass] = boost.predict_proba(test_x)[:,i_label] \n",
    "    # pd.DataFrame(ordinal_preds)\n",
    "    return boost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abe2a665-096f-4646-9134-1e611bb224b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entreno_todos_algoritmos_conozco():\n",
    "    # Entreno todos los algoritmos con el set de caracteristicas para y las etiquetas\n",
    "    # ALG.1\n",
    "    # dTree = algoritmo_DecisionTreeClassifier_train_max_depth (x_train, y_train,max_depth=5,min_samples_leaf=3)\n",
    "    dTree = algoritmo_DecisionTreeClassifier_train_max_depth (x_train, y_train,max_depth=5)\n",
    "    # ALG.2\n",
    "    knn = algoritmo_KNeighborsClassifier (x_train, y_train)\n",
    "    # ALG.3\n",
    "    nbayes = algoritmo_GaussianNB (x_train, y_train)\n",
    "    # ALG.4\n",
    "    logreg = model_LogisticRegression (x_train, y_train)\n",
    "    # ALG.5\n",
    "    bagg_dTree = algoritmo_BaggingClassifier (dTree, x_train, y_train, n_estimators = 10, random_state=0)\n",
    "    # ALG.6\n",
    "    bagg_knn = algoritmo_BaggingClassifier (knn, x_train, y_train, n_estimators = 10, random_state=0)\n",
    "    # ALG.7\n",
    "    bagg_nbayes = algoritmo_BaggingClassifier (nbayes, x_train, y_train, n_estimators = 10, random_state=0)\n",
    "    # ALG.8\n",
    "    bagg_logreg = algoritmo_BaggingClassifier (logreg, x_train, y_train, n_estimators = 10, random_state=0)\n",
    "    # ALG.9\n",
    "    #Definimos un espacio de parámetros.  Cada prueba consistirá en una combinación de estos parámetros posibles\n",
    "    param_grid = {'max_features': [2, 3, 5, 8],'n_estimators': [20, 50, 100]}\n",
    "    grid_cv, best_rf, best_params = algoritmo_RandomForestClassifier (x_train, y_train,1, 5 ,features, target,param_grid,3,20)\n",
    "    # ALG.10\n",
    "    boost = algoritmo_AdaBoostClassifier (x_train, y_train,n_estimators=3)\n",
    "    # ALG.11\n",
    "    bagg_best_rf = algoritmo_BaggingClassifier (best_rf, x_train, y_train, n_estimators = 10, random_state=0)\n",
    "    # ALG.12\n",
    "    bagg_boost = algoritmo_BaggingClassifier (boost, x_train, y_train, n_estimators = 10, random_state=0)\n",
    "    \n",
    "    return dTree,knn,nbayes,logreg,bagg_dTree,bagg_knn,bagg_nbayes,bagg_logreg,best_rf,boost,bagg_best_rf,bagg_boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00add99e-7630-4656-b883-c0278d3cde0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_matrices_metricas_y_graficas(x_test, y_test, x_train, y_train):\n",
    "    #guardo en test_predicted_resumen todos los test predecidos\n",
    "    import pandas as pd\n",
    "    test_predicted_resumen = pd.DataFrame(y_test, columns = ['label'])\n",
    "\n",
    "    test_predicted = dTree.predict(x_test)\n",
    "    test_predicted_resumen['dTree'] = test_predicted\n",
    "    test_predicted = knn.predict(x_test)\n",
    "    test_predicted_resumen['knn'] = test_predicted\n",
    "    test_predicted = nbayes.predict(x_test)\n",
    "    test_predicted_resumen['nbayes'] = test_predicted\n",
    "    test_predicted = logreg.predict(x_test)\n",
    "    test_predicted_resumen['logreg'] = test_predicted\n",
    "    test_predicted = bagg_dTree.predict(x_test)\n",
    "    test_predicted_resumen['bagg_dTree'] = test_predicted\n",
    "    test_predicted = bagg_knn.predict(x_test)\n",
    "    test_predicted_resumen['bagg_knn'] = test_predicted\n",
    "    test_predicted = bagg_nbayes.predict(x_test)\n",
    "    test_predicted_resumen['bagg_nbayes'] = test_predicted\n",
    "    test_predicted = bagg_logreg.predict(x_test)\n",
    "    test_predicted_resumen['bagg_logreg'] = test_predicted\n",
    "    test_predicted = best_rf.predict(x_test)\n",
    "    test_predicted_resumen['best_rf'] = test_predicted\n",
    "    test_predicted = boost.predict(x_test)\n",
    "    test_predicted_resumen['boost'] = test_predicted\n",
    "    test_predicted = bagg_best_rf.predict(x_test)\n",
    "    test_predicted_resumen['bagg_best_rf'] = test_predicted\n",
    "    test_predicted = bagg_boost.predict(x_test)\n",
    "    test_predicted_resumen['bagg_boost'] = test_predicted\n",
    "    #test_predicted_resumen\n",
    "\n",
    "    matriz_confusion, matriz_confusion_df = metrica_confusion_matrix(dTree, x_test, y_test)\n",
    "    matriz_confusion_df_resumen  = matriz_confusion_df\n",
    "    matriz_confusion_df_resumen.columns =['dTree_no','dTree_si']\n",
    "    matriz_confusion, matriz_confusion_df = metrica_confusion_matrix(knn, x_test, y_test)\n",
    "    matriz_confusion_df_resumen['knn_no'] = matriz_confusion[0]\n",
    "    matriz_confusion_df_resumen['knn_si'] = matriz_confusion[1]\n",
    "    matriz_confusion, matriz_confusion_df = metrica_confusion_matrix(nbayes, x_test, y_test)\n",
    "    matriz_confusion_df_resumen['nbayes_no'] = matriz_confusion[0]\n",
    "    matriz_confusion_df_resumen['nbayes_si'] = matriz_confusion[1]\n",
    "    matriz_confusion, matriz_confusion_df = metrica_confusion_matrix(logreg, x_test, y_test)\n",
    "    matriz_confusion_df_resumen['logreg_no'] = matriz_confusion[0]\n",
    "    matriz_confusion_df_resumen['logreg_si'] = matriz_confusion[1]\n",
    "    matriz_confusion, matriz_confusion_df = metrica_confusion_matrix(bagg_dTree, x_test, y_test)\n",
    "    matriz_confusion_df_resumen['bagg_dTree_no'] = matriz_confusion[0]\n",
    "    matriz_confusion_df_resumen['bagg_dTree_si'] = matriz_confusion[1]\n",
    "    matriz_confusion, matriz_confusion_df = metrica_confusion_matrix(bagg_knn, x_test, y_test)\n",
    "    matriz_confusion_df_resumen['bagg_knn_no'] = matriz_confusion[0]\n",
    "    matriz_confusion_df_resumen['bagg_knn_si'] = matriz_confusion[1]\n",
    "    matriz_confusion, matriz_confusion_df = metrica_confusion_matrix(bagg_nbayes, x_test, y_test)\n",
    "    matriz_confusion_df_resumen['bagg_nbayes_no'] = matriz_confusion[0]\n",
    "    matriz_confusion_df_resumen['bagg_nbayes_si'] = matriz_confusion[1]\n",
    "    matriz_confusion, matriz_confusion_df = metrica_confusion_matrix(bagg_logreg, x_test, y_test)\n",
    "    matriz_confusion_df_resumen['bagg_logreg_no'] = matriz_confusion[0]\n",
    "    matriz_confusion_df_resumen['bagg_logreg_si'] = matriz_confusion[1]\n",
    "    matriz_confusion, matriz_confusion_df = metrica_confusion_matrix(best_rf, x_test, y_test)\n",
    "    matriz_confusion_df_resumen['best_rf_no'] = matriz_confusion[0]\n",
    "    matriz_confusion_df_resumen['best_rf_si'] = matriz_confusion[1]\n",
    "    matriz_confusion, matriz_confusion_df = metrica_confusion_matrix(boost, x_test, y_test)\n",
    "    matriz_confusion_df_resumen['boost_no'] = matriz_confusion[0]\n",
    "    matriz_confusion_df_resumen['boost_si'] = matriz_confusion[1]\n",
    "    matriz_confusion, matriz_confusion_df = metrica_confusion_matrix(bagg_best_rf, x_test, y_test)\n",
    "    matriz_confusion_df_resumen['bagg_best_rf_no'] = matriz_confusion[0]\n",
    "    matriz_confusion_df_resumen['bagg_best_rf_si'] = matriz_confusion[1]\n",
    "    matriz_confusion, matriz_confusion_df = metrica_confusion_matrix(bagg_boost, x_test, y_test)\n",
    "    matriz_confusion_df_resumen['bagg_boost_no'] = matriz_confusion[0]\n",
    "    matriz_confusion_df_resumen['bagg_boost_si'] = matriz_confusion[1]\n",
    "\n",
    "    columns = ['dTree', 'knn','nbayes', 'logreg','bagg_dTree', 'bagg_knn','bagg_nbayes', 'bagg_logreg','best_rf', 'boost','bagg_best_rf', 'bagg_boost']\n",
    "    metrica_accuracy_score_resumen = pd.DataFrame(columns = columns)\n",
    "    metrica_accuracy_score_resumen.loc['metrica_accuracy_score','dTree'] = metrica_accuracy_score (dTree, x_test, y_test)\n",
    "    metrica_accuracy_score_resumen.loc['metrica_accuracy_score','knn'] = metrica_accuracy_score (knn, x_test, y_test)\n",
    "    metrica_accuracy_score_resumen.loc['metrica_accuracy_score','nbayes'] = metrica_accuracy_score (nbayes, x_test, y_test)\n",
    "    metrica_accuracy_score_resumen.loc['metrica_accuracy_score','logreg'] = metrica_accuracy_score (logreg, x_test, y_test)\n",
    "    metrica_accuracy_score_resumen.loc['metrica_accuracy_score','bagg_dTree'] = metrica_accuracy_score (bagg_dTree, x_test, y_test)\n",
    "    metrica_accuracy_score_resumen.loc['metrica_accuracy_score','bagg_knn'] = metrica_accuracy_score (bagg_knn, x_test, y_test)\n",
    "    metrica_accuracy_score_resumen.loc['metrica_accuracy_score','bagg_nbayes'] = metrica_accuracy_score (bagg_nbayes, x_test, y_test)\n",
    "    metrica_accuracy_score_resumen.loc['metrica_accuracy_score','bagg_logreg'] = metrica_accuracy_score (bagg_logreg, x_test, y_test)\n",
    "    metrica_accuracy_score_resumen.loc['metrica_accuracy_score','best_rf'] = metrica_accuracy_score (best_rf, x_test, y_test)\n",
    "    metrica_accuracy_score_resumen.loc['metrica_accuracy_score','boost'] = metrica_accuracy_score (boost, x_test, y_test)\n",
    "    metrica_accuracy_score_resumen.loc['metrica_accuracy_score','bagg_best_rf'] = metrica_accuracy_score (bagg_best_rf, x_test, y_test)\n",
    "    metrica_accuracy_score_resumen.loc['metrica_accuracy_score','bagg_boost'] = metrica_accuracy_score (bagg_boost, x_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "    index = ['0_fit_time', '0_score_time', '0_test_accuracy', '0_test_roc_auc',\n",
    "             '1_fit_time', '1_score_time', '1_test_accuracy', '1_test_roc_auc',\n",
    "             '2_fit_time', '2_score_time', '2_test_accuracy', '2_test_roc_auc',\n",
    "             '3_fit_time', '3_score_time', '3_test_accuracy', '3_test_roc_auc',\n",
    "             'mean_fit_time', 'mean_score_time', 'mean_test_accuracy', 'mean_test_roc_auc',\n",
    "             'Kfold_0_auc', 'Kfold_0_accuracy', 'Kfold_1_auc', 'Kfold_1_accuracy',\n",
    "             'Kfold_2_auc', 'Kfold_2_accuracy', 'Kfold_3_auc', 'Kfold_3_accuracy',\n",
    "             'Kfold_mean_auc', 'Kfold_mean_accuracy', \n",
    "             'precision_score_micro','precision_score_macro','precision_score_ninguno','precision_score_tao',\n",
    "             'r2_score(y_test, y_pred)','r2_score(y_train, y_pred_train)',\n",
    "             'roc_auc_score_test', 'roc_auc_score_train',\n",
    "             'accuracy(Precision_global)_FERNANDO', 'precision(Precision_de_clase)_FERNANDO', 'recall(Sensibilidad)_FERNANDO', \n",
    "             'speciﬁcity(Especiﬁcidad)_FERNANDO', 'Ratio_de_falsos_positivos_(1-speciﬁcity)_FERNANDO']        \n",
    "\n",
    "    columns = ['dTree', 'knn','nbayes', 'logreg','bagg_dTree', 'bagg_knn','bagg_nbayes', 'bagg_logreg','best_rf', 'boost','bagg_best_rf', 'bagg_boost']\n",
    "    metrica_all_SCORERS_keys_y_model_Kfold_resumen = pd.DataFrame(index = index, columns = columns)\n",
    "    columns_2 = [dTree, knn,nbayes, logreg,bagg_dTree, bagg_knn,bagg_nbayes, bagg_logreg,best_rf, boost,bagg_best_rf, bagg_boost]\n",
    "    col = 0\n",
    "    for i in columns_2:\n",
    "        a, b = metrica_all_SCORERS_keys (i, features, target)\n",
    "        c, d = model_Kfold (dTree, features, target, n_splits = 4)\n",
    "        for i in range(0,4):\n",
    "            metrica_all_SCORERS_keys_y_model_Kfold_resumen.iloc[i,col] = a.iloc[0,i]\n",
    "            metrica_all_SCORERS_keys_y_model_Kfold_resumen.iloc[i+4,col] = a.iloc[1,i]\n",
    "            metrica_all_SCORERS_keys_y_model_Kfold_resumen.iloc[i+8,col] = a.iloc[2,i]\n",
    "            metrica_all_SCORERS_keys_y_model_Kfold_resumen.iloc[i+12,col] = a.iloc[3,i]       \n",
    "            metrica_all_SCORERS_keys_y_model_Kfold_resumen.iloc[i+16,col] = b[i]\n",
    "        for j in range(0,2): \n",
    "            metrica_all_SCORERS_keys_y_model_Kfold_resumen.iloc[j+20,col] = c.iloc[0,j]\n",
    "            metrica_all_SCORERS_keys_y_model_Kfold_resumen.iloc[j+22,col] = c.iloc[1,j]\n",
    "            metrica_all_SCORERS_keys_y_model_Kfold_resumen.iloc[j+24,col] = c.iloc[2,j]\n",
    "            metrica_all_SCORERS_keys_y_model_Kfold_resumen.iloc[j+26,col] = c.iloc[3,j]\n",
    "            metrica_all_SCORERS_keys_y_model_Kfold_resumen.iloc[j+28,col] = d[j]\n",
    "\n",
    "        col = col+ 1\n",
    "\n",
    "\n",
    "    columns = ['dTree', 'knn','nbayes', 'logreg','bagg_dTree', 'bagg_knn','bagg_nbayes', 'bagg_logreg','best_rf', 'boost','bagg_best_rf', 'bagg_boost']\n",
    "    col=0\n",
    "    for i in columns :\n",
    "        micro, macro, ninguno, tao = metrica_precision_score (y_test, test_predicted_resumen[i])\n",
    "        metrica_all_SCORERS_keys_y_model_Kfold_resumen.iloc[30,col] = micro\n",
    "        metrica_all_SCORERS_keys_y_model_Kfold_resumen.iloc[31,col] = macro\n",
    "        metrica_all_SCORERS_keys_y_model_Kfold_resumen.iloc[32,col] = ninguno\n",
    "        metrica_all_SCORERS_keys_y_model_Kfold_resumen.iloc[33,col] = tao\n",
    "        col = col + 1\n",
    "\n",
    "\n",
    "    columns_2 = [dTree, knn,nbayes, logreg,bagg_dTree, bagg_knn,bagg_nbayes, bagg_logreg,best_rf, boost,bagg_best_rf, bagg_boost]\n",
    "    col=0\n",
    "    for i in columns_2:\n",
    "        a, b = metrica_r2_score (i, x_test, y_test, x_train, y_train)\n",
    "        metrica_all_SCORERS_keys_y_model_Kfold_resumen.iloc[34,col] = a\n",
    "        metrica_all_SCORERS_keys_y_model_Kfold_resumen.iloc[35,col] = b\n",
    "        c, d = metrica_roc_auc_score (i, x_test, y_test, x_train, y_train)\n",
    "        metrica_all_SCORERS_keys_y_model_Kfold_resumen.iloc[36,col] = c\n",
    "        metrica_all_SCORERS_keys_y_model_Kfold_resumen.iloc[37,col] = d    \n",
    "        col = col + 1\n",
    "\n",
    "\n",
    "    columns = ['dTree', 'knn','nbayes', 'logreg','bagg_dTree', 'bagg_knn','bagg_nbayes', 'bagg_logreg','best_rf', 'boost','bagg_best_rf', 'bagg_boost']\n",
    "    col=0\n",
    "    for i in range(0,matriz_confusion_df_resumen.shape[1],2):\n",
    "        mc = matriz_confusion_df_resumen.iloc[:,i:i+2]\n",
    "        TN = mc.iloc[0,0]\n",
    "        FP = mc.iloc[1,0]\n",
    "        FN = mc.iloc[0,1]\n",
    "        TP = mc.iloc[1,1] \n",
    "        P = TP + FN\n",
    "        N = TN + FP \n",
    "        acc = (TP + TN) / (P + N)\n",
    "        prec = (TP) / (TP + FP)\n",
    "        rec = TP / P\n",
    "        spe = TN / N\n",
    "        rfp = FP / N\n",
    "        metrica_all_SCORERS_keys_y_model_Kfold_resumen.iloc[38,col] = acc\n",
    "        metrica_all_SCORERS_keys_y_model_Kfold_resumen.iloc[39,col] = prec\n",
    "        metrica_all_SCORERS_keys_y_model_Kfold_resumen.iloc[40,col] = rec\n",
    "        metrica_all_SCORERS_keys_y_model_Kfold_resumen.iloc[41,col] = spe\n",
    "        metrica_all_SCORERS_keys_y_model_Kfold_resumen.iloc[42,col] = rfp\n",
    "        col = col + 1\n",
    "    metrica_all_SCORERS_keys_y_model_Kfold_resumen\n",
    "\n",
    "\n",
    "    # tit=0\n",
    "    # for i in columns_2:\n",
    "    #     metrica_roc_curve (i, x_test, y_test, x_train,y_train, pos_label= 'yes',titulo =columns[tit])\n",
    "    #     tit = tit +1\n",
    "\n",
    "    return test_predicted_resumen, matriz_confusion_df_resumen, metrica_accuracy_score_resumen, metrica_all_SCORERS_keys_y_model_Kfold_resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b5131c2-9f2d-42cf-9b26-c8eb8fd17c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrica_roc_curve_pdf (df_para_metrica, x_test, y_test, x_train,y_train, pos_label,titulo):\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    import matplotlib.pyplot as plt\n",
    "    test_pred = df_para_metrica.predict_proba(x_test) #variable\n",
    "    test_posprob = test_pred[:,1]\n",
    "\n",
    "    train_pred = df_para_metrica.predict_proba(x_train) #variable\n",
    "    train_posprob = train_pred[:,1]\n",
    "    \n",
    "    \n",
    "    train_fpr, train_tpr, train_thr = roc_curve(y_train, train_posprob, pos_label=pos_label) #valores de etiqueta en la def\n",
    "    test_fpr, test_tpr, test_thr = roc_curve(y_test, test_posprob, pos_label=pos_label) #valores de etiqueta en la def\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    plt.plot(train_fpr, train_tpr, color='r', label='train')\n",
    "    plt.plot(test_fpr, test_tpr, color='g', label='test')\n",
    "\n",
    "    plt.plot([0,1],[0,1])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0,1.0])\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title(titulo)\n",
    "    \n",
    "    nombre = f'{titulo}.pdf'\n",
    "    fig.savefig(nombre, format='pdf')\n",
    "    \n",
    "    return\n",
    "\n",
    "def guardar_todas_graficas_ROC_en_un_pdf (x_test, y_test, x_train,y_train):\n",
    "    columns_2 = [dTree, knn,nbayes, logreg,bagg_dTree, bagg_knn,bagg_nbayes, bagg_logreg,best_rf, boost,bagg_best_rf, bagg_boost]\n",
    "    columns = ['dTree', 'knn','nbayes', 'logreg','bagg_dTree', 'bagg_knn','bagg_nbayes', 'bagg_logreg','best_rf', 'boost','bagg_best_rf', 'bagg_boost']\n",
    "    tit=0\n",
    "    for i in columns_2:\n",
    "        metrica_roc_curve_pdf (i, x_test, y_test, x_train,y_train, pos_label= 'yes',titulo =columns[tit])    \n",
    "        tit = tit +1\n",
    "\n",
    "    from PyPDF2 import PdfMerger\n",
    "    pdfs = []\n",
    "    tit=0\n",
    "    for i in range(0,len(columns_2)):\n",
    "        titulo =columns[tit]\n",
    "        pdfs.append(f'{titulo}.pdf')\n",
    "        tit = tit +1\n",
    "    pdfs\n",
    "\n",
    "    merger = PdfMerger()\n",
    "\n",
    "    for pdf in pdfs:\n",
    "        merger.append(pdf)\n",
    "\n",
    "    merger.write(\"Graficas_Ej_5_1_ROC_Todas.pdf\")\n",
    "    merger.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d8a91db-a527-43d8-842f-c2cc91816f6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'knn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 35>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m     plt\u001b[38;5;241m.\u001b[39mxticks(rotation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m90\u001b[39m)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m algoritmo_1 \u001b[38;5;241m=\u001b[39m \u001b[43mknn\u001b[49m\n\u001b[0;32m     36\u001b[0m algoritmo_2 \u001b[38;5;241m=\u001b[39m dTree\n\u001b[0;32m     37\u001b[0m legenda \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTree\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKnn\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'knn' is not defined"
     ]
    }
   ],
   "source": [
    "# para el ejer5_2\n",
    "def inspection_permutation_importance (algoritmo_1, algoritmo_2, x_train, y_train,legenda):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    pimp_algoritmo_1 = permutation_importance(algoritmo_1, x_train, y_train, n_repeats=10)\n",
    "    pimp_algoritmo_2 = permutation_importance(algoritmo_2, x_train, y_train, n_repeats=10)\n",
    "    imp_df = pd.DataFrame({'ft': features.columns,\n",
    "                       'algoritmo_2_mean': pimp_algoritmo_2.importances_mean,\n",
    "                       'algoritmo_2_std': pimp_algoritmo_2.importances_std,\n",
    "                       'algoritmo_1_mean': pimp_algoritmo_1.importances_mean,\n",
    "                       'algoritmo_1_std': pimp_algoritmo_1.importances_std\n",
    "                      })\n",
    "    imp_df.sort_values('algoritmo_2_mean', ascending=False, inplace=True)\n",
    "    imp_df_better = imp_df.iloc[:10]\n",
    "    \n",
    "    bw=0.25\n",
    "    plt.bar(np.arange(imp_df_better.shape[0]),\n",
    "                      imp_df_better.algoritmo_2_mean,\n",
    "                      yerr=imp_df_better.algoritmo_2_std,\n",
    "                      width=bw,\n",
    "                      tick_label=imp_df_better.ft,\n",
    "                      )\n",
    "    plt.bar(np.arange(imp_df_better.shape[0])+bw,\n",
    "                      imp_df_better.algoritmo_1_mean,\n",
    "                      yerr=imp_df_better.algoritmo_1_std,\n",
    "                      width=bw,\n",
    "                      tick_label=imp_df_better.ft)\n",
    "    plt.legend(legenda)\n",
    "    plt.xticks(rotation=90)\n",
    "    return\n",
    "\n",
    "\n",
    "algoritmo_1 = knn\n",
    "algoritmo_2 = dTree\n",
    "legenda = ['Tree','Knn']\n",
    "inspection_permutation_importance (algoritmo_1, algoritmo_2, x_train, y_train,legenda)\n",
    "\n",
    "\n",
    "#7_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b0cd26-bea5-4165-994d-524225ecb01c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
